# JailbreakingZoo: Survey, Landscapes, and Horizons in the Jailbreaking of Large Models

## Introduction

Welcome to JailbreakingZoo, a dedicated repository focused on the jailbreaking of large models (LMs), encompassing both large language models (LLMs) and vision language models (VLMs). This project aims to explore the vulnerabilities, exploit methods, and defense mechanisms associated with these advanced AI models. Our goal is to foster a deeper understanding and awareness of the security aspects surrounding large-scale AI systems.

## Timeline

This repository's contents are systematically organized according to a timeline, with the latest update being March 14, 2024.


## Contents

- [**Jailbreaks of LLMs**](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/Papers/LLM_Jailbreak.md): Discover the techniques and case studies related to the jailbreaking of large language models.

- [**Defenses of LLMs**](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/Papers/LLM_Defense.md): Explore the strategies and methods employed to defend large language models against various types of attacks.

- [**Jailbreaks of VLMs**](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/Papers/VLM_Jailbreak.md): Learn about the vulnerabilities and jailbreaking approaches specific to vision language models.

- [**Defenses of VLMs**](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/Papers/VLM_Defense.md): Understand the defense mechanisms designed for vision language models, including the most recent advancements and strategies.

## Badges

- Jailbreaks of LLMs: ![Gradient-based](https://img.shields.io/badge/-Gradient--based-blue) ![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen) ![Prompt-based](https://img.shields.io/badge/-Prompt--based-red) ![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange) ![Query-based](https://img.shields.io/badge/-Query--based-lightgrey) ![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen) ![Survey](https://img.shields.io/badge/-Survey-ff69b4)

- Access: ![Blackbox](https://img.shields.io/badge/-Blackbox-black) ![Whitebox](https://img.shields.io/badge/-Whitebox-white)

## Contributing

We welcome contributions from the community! Whether you're interested in adding new research, improving existing documentation, or sharing your own jailbreak or defense strategies, your insights are valuable to us. Please check our [Contribution Guidelines](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/CONTRIBUTING.md) for more information on how you can get involved.

## License and Citation

This project is available under the [MIT License](https://github.com/Allen-piexl/JailbreakingZoo/blob/main/LICENSE). Please refer to our citation guidelines if you wish to reference our work in your research or publications.

Thank you for visiting JailbreakingZoo. We hope this repository serves as a valuable resource in your exploration of large model security.

## Acknowledgement

Special thanks to our notable contributors: [**Haibo Jin**](https://github.com/Allen-piexl/), [**Leyang Hu**](https://github.com/Leon-Leyang), [**Xinuo Li**](https://github.com/monmonli), [**Peiyan Zhang**](https://github.com/Peiyance), [**Chonghan Chen**](https://github.com/PaulCCCCCCH), [**Jun Zhuang**](https://github.com/junzhuang-code), and [**Haohan Wang**](https://github.com/HaohanWang). 

*The ranking is in partial order.
