# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-03-14] 🌱 AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting [Paper](https://arxiv.org/pdf/2402.08567.pdf)][[Code](https://github.com/rain305f/AdaShield)]

[2024-02-03] 💕 Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models [[Paper](https://arxiv.org/pdf/2402.02207.pdf)][[Code](https://github.com/ys-zong/VLGuard)]

[2023-12-17] 🎄 A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection [[Paper](https://arxiv.org/pdf/2312.10766.pdf)][[Code](https://github.com/shiningrain/JailGuard)]

[2023-11-29] 🦃 MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models [[Paper](https://arxiv.org/pdf/2311.17600.pdf)]

[2023-11-15] 🦃 Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts [[Paper](https://arxiv.org/pdf/2311.09127.pdf)]

[2023-10-03] 🎃 Low-Resource Languages Jailbreak GPT-4 [[Paper](https://arxiv.org/pdf/2310.02446.pdf)]

[2023-09-25] 🍂 SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution [[Paper](https://arxiv.org/pdf/2309.14122.pdf)][[Code](https://github.com/Zjm1900/SurrogatePrompt)]

[2023-08-12] 🌴 GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher [[Paper](https://arxiv.org/pdf/2308.06463.pdf)][[Code](https://github.com/RobustNLP/CipherChat)]

[2023-05-20] 🦃 SneakyPrompt: Jailbreaking Text-to-image Generative Models [[Paper](https://arxiv.org/pdf/2305.12082.pdf)][[Code](https://github.com/Yuchen413/text2image_safety)]

