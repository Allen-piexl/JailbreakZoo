# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-03-12] 🌱 Exploring Safety Generalization Challenges of Large Language Models via Code [[Paper](https://arxiv.org/pdf/2403.07865.pdf)]

[2024-02-28] 💕 Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction [[Paper](https://arxiv.org/pdf/2402.18104.pdf)][[Code](https://sites.google.com/view/dra-jailbreak/)]

[2024-02-26] 💕 CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/abs/2402.16717.pdf)][[Code](https://github.com/huizhang-L/CodeChameleon)]

[2024-02-25] 💕 From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings [[Paper](https://arxiv.org/abs/2402.16006.pdf)]

[2024-02-25] 💕 DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers [[Paper](https://arxiv.org/pdf/2402.16914.pdf)][[Code](https://github.com/xirui-li/DrAttack)]

[2024-02-24] 💕 PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails [[Paper](https://arxiv.org/pdf/2402.15911.pdf)]

[2024-02-23] 💕 How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries [[Paper](https://arxiv.org/abs/2402.15302.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA)]

[2024-02-21] 💕 Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs [[Paper](https://arxiv.org/abs/2402.14872.pdf)]

[2024-02-20] 💕 Is the System Message Really Important to Jailbreaks in Large Language Models? [[Paper](https://arxiv.org/abs/2402.14857.pdf)]

[2024-02-21] 💕 LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper](https://arxiv.org/abs/2402.13457.pdf)][[Code](https://sites.google.com/view/llmcomprehensive/home)]

[2024-02-21] 💕 Coercing LLMs to do and reveal (almost) anything [[Paper](https://arxiv.org/abs/2402.14020.pdf)][[Code](https://github.com/JonasGeiping/carving)]

[2024-02-19] 💕 Query-Based Adversarial Prompt Generation [[Paper](https://arxiv.org/abs/2402.12329.pdf)]

[2024-02-19] 💕 ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs [[Paper](https://arxiv.org/abs/2402.11753.pdf)][[Code](https://github.com/uw-nsl/ArtPrompt)]

[2024-02-19] 💕 SPML: A DSL for Defending Language Models Against Prompt Attacks [[Paper](https://arxiv.org/abs/2402.11755.pdf)][[Code](https://prompt-compiler.github.io/SPML/)]

[2024-02-16] 💕 Jailbreaking Proprietary Large Language Models using Word Substitution Cipher [[Paper](https://arxiv.org/abs/2402.10601.pdf)]

[2024-02-16] 💕 ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages [[Paper](https://arxiv.org/abs/2402.10753.pdf)][[Code](https://github.com/Junjie-Ye/ToolSword)]

[2024-02-15] 💕 A StrongREJECT for Empty Jailbreaks [[Paper](https://arxiv.org/abs/2402.10260.pdf)][[Code](https://github.com/alexandrasouly/strongreject)]

[2024-02-15] 💕 PAL: Proxy-Guided Black-Box Attack on Large Language Models [[Paper](https://arxiv.org/abs/2402.09674.pdf)][[Code](https://github.com/chawins/pal)]

[2024-02-14] 💕 Attacking Large Language Models with Projected Gradient Descent [[Paper](https://arxiv.org/abs/2402.09154.pdf)]

[2024-02-14] 💕 Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues [[Paper](https://arxiv.org/abs/2402.09091.pdf)]

[2024-02-13] 💕 COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability [[Paper](https://arxiv.org/abs/2402.08679.pdf)][[Code](https://github.com/Yu-Fangxu/COLD-Attack)]

[2024-02-13] 💕 Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning [[Paper](https://arxiv.org/abs/2402.08416.pdf)]

[2024-02-08] 💕 Comprehensive Assessment of Jailbreak Attacks Against LLMs [[Paper](https://arxiv.org/abs/2402.05668.pdf)]

[2024-02-06] 💕 HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal [[Paper](https://arxiv.org/abs/2402.04249.pdf)][[Code](https://github.com/centerforaisafety/HarmBench)]

[2024-02-05] 💕 GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models [[Paper](https://arxiv.org/abs/2402.03299.pdf)][[Code](https://github.com/Allen-piexl/GUARD)]

[2024-01-30] ❄️ A Cross-Language Investigation into Jailbreak Attacks in Large Language Models [[Paper](https://arxiv.org/pdf/2401.16765.pdf)]

[2024-01-30] ❄️ Weak-to-Strong Jailbreaking on Large Language Models [[Paper](https://arxiv.org/abs/2401.17256.pdf)][[Code](https://github.com/XuandongZhao/weak-to-strong)]

[2024-01-22] ❄️ PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety [[Paper](https://arxiv.org/abs/2401.11880.pdf)][[Code](https:/github.com/AI4Good24/PsySafe)]

[2024-01-19] ❄️ Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models [[Paper](https://arxiv.org/abs/2401.10647.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA)]

[2024-01-18] ❄️ All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks [[Paper](https://arxiv.org/abs/2401.09798.pdf)][[Code](https://github.com/kztakemoto/simbaja)]

[2024-01-17] ❄️ AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models [[Paper](https://arxiv.org/abs/2401.09002.pdf)][[Code](https://github.com/BWangCN/AttackEval)]

[2024-01-12] ❄️ How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs [[Paper](https://arxiv.org/pdf/2401.06373.pdf)][[Code](https://github.com/CHATS-lab/persuasive_jailbreaker)]

[2023-12-18] 🎄 A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models [[Paper](https://arxiv.org/abs/2312.10982.pdf)]

[2023-12-07] 🎄 Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak [[Paper](https://arxiv.org/abs/2312.04127.pdf)]

[2023-12-04] 🎄 Tree of Attacks: Jailbreaking Black-Box LLMs Automatically [[Paper](https://arxiv.org/abs/2312.02119.pdf)]
[2023-12] 🌱 Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack [[Paper](https://arxiv.org/abs/2312.06924)]
[2023-12] 🛡 A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection [[Paper](https://arxiv.org/abs/2312.10766)]
[2023-12] 🌱 Adversarial Attacks on GPT-4 via Simple Random Search [[Paper](https://www.andriushchenko.me/gpt4adv.pdf)]

[2023-12-08] 🎄 Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs [[Paper](https://arxiv.org/abs/2312.04782.pdf)][[Code](https://img.shields.io/badge/CodeGen-87b800)]

[2023-11-21] 🦃 Goal-Oriented Prompt Attack and Safety Evaluation for LLMs [[Paper](https://arxiv.org/abs/2309.11830.pdf)][[Code](https://github.com/liuchengyuan123/CPAD)]


[2023-11] 🌱 Query-Relevant Images Jailbreak Large Multi-Modal Models [[Paper](https://arxiv.org/abs/2311.17600)]
[2023-11] 🌱 A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts Can Fool Large Language Models Easily [[Paper](https://arxiv.org/abs/2311.08268)]
[2023-11] 🌱 Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles [[Paper](https://arxiv.org/abs/2311.14876)]
[2023-11] 🌱 Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks [[Paper](https://arxiv.org/abs/2302.05733)]
[2023-11] 🌱 MART: Improving LLM Safety with Multi-round Automatic Red-Teaming [[Paper](https://arxiv.org/abs/2311.07689)]
[2023-11] 🌱 Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation [[Paper](https://arxiv.org/abs/2311.03348)]
[2023-11] 🛡 SneakyPrompt: Jailbreaking Text-to-image Generative Models [[Paper](https://arxiv.org/abs/2305.12082)][[Code](https://github.com/Yuchen413/text2image_safety)]
[2023-11] 🌱 DeepInception: Hypnotize Large Language Model to Be Jailbreaker [[Paper](https://arxiv.org/abs/2311.03191)][[Code](https://github.com/tmlr-group/DeepInception?tab=readme-ov-file)]
[2023-11] 🌱 Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition [[Paper](https://arxiv.org/abs/2311.16119)]
[2023-11] 🌱 Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild [[Paper](https://arxiv.org/abs/2311.06237)]
[2023-11] 🛡 Evil Geniuses: Delving into the Safety of LLM-based Agents [[Paper](https://arxiv.org/abs/2311.11855)][[Code](https://github.com/evil-genius-project/evil-genius)]
[2023-11] 🌱 FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts [[Paper](https://arxiv.org/abs/2311.05608)][[Code](https://github.com/ThuCCSLab/FigStep)]

[2023-05-24] 🌺 Adversarial Demonstration Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2305.14950.pdf)]

[2023-05-24] 🌺 Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks [[Paper](https://arxiv.org/pdf/2305.14965.pdf)][[Code](https://github.com/AetherPrior/TrickLLM)]

[2023-05-23] 🌺 Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [[Paper](https://arxiv.org/pdf/2305.13860.pdf)][[Code](https://sites.google.com/view/llm-jailbreak-study)]

[2023-04-11] 🌸 Multi-step Jailbreaking Privacy Attacks on ChatGPT [[Paper](https://arxiv.org/pdf/2304.05197.pdf)][[Code](https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak)]

[2023-03-08] 🌱 Automatically Auditing Large Language Models via Discrete Optimization [[Paper](https://proceedings.mlr.press/v202/jones23a/jones23a.pdf)][[Code](https://github.com/ejones313/auditing-llms)]
