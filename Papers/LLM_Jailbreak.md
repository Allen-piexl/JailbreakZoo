# Timeline ğŸš€ 
(Jan:â„ï¸, Feb:ğŸ’•, Mar:ğŸŒ±, Apr:ğŸŒ¸, May:ğŸŒº, Jun:â˜€ï¸, Jul:ğŸ¦, Aug:ğŸŒ´, Sep:ğŸ‚, Oct:ğŸƒ, Nov:ğŸ¦ƒ, Dec:ğŸ„)

[2024-08-21] ğŸŒ´ Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer [[Paper](https://arxiv.org/pdf/2408.11313)][[Code](https://github.com/lenijwp/ECLIPSE)]

[2024-08-20] ğŸŒ´ Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles [[Paper](https://arxiv.org/pdf/2408.11182)]

[2024-08-20] ğŸŒ´ Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation [[Paper](https://arxiv.org/pdf/2408.10668)]

[2024-08-14] ğŸŒ´ SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming [[Paper](https://arxiv.org/pdf/2408.11851)]

[2024-08-09] ğŸŒ´ Jailbreak Open-Sourced Large Language Models via Enforced Decoding [[Paper](https://aclanthology.org/2024.acl-long.299.pdf)]

[2024-08-09] ğŸŒ´ h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment [[Paper](https://arxiv.org/pdf/2408.04811)]

[2024-08-09] ğŸŒ´ EnJa: Ensemble Jailbreak on Large Language Models [[Paper](https://arxiv.org/pdf/2408.05061)][[Code](https://sites.google.com/view/promptware/home)]

[2024-08-07] ğŸŒ´ EnJa: Ensemble Jailbreak on Large Language Models [[Paper](https://arxiv.org/pdf/2408.03603)]

[2024-07-25] ğŸ¦ Exploring Scaling Trends in LLM Robustness [[Paper](https://arxiv.org/pdf/2407.18213)]

[2024-07-25] ğŸ¦ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2407.17915)][[Code](https://github.com/wooozihui/jailbreakfunction)]

[2024-07-23] ğŸ¦ Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models [[Paper](https://arxiv.org/pdf/2407.16205)]

[2024-07-23] ğŸ¦ RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent [[Paper](https://arxiv.org/pdf/2407.16667)]

[2024-07-22] ğŸ¦ Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models [[Paper](https://arxiv.org/pdf/2407.15399)]

[2024-07-19] ğŸ¦ Does Refusal Training in LLMs Generalize to the Past Tense? [[Paper](https://arxiv.org/pdf/2407.11969)][[Code](https://github.com/tml-epfl/llm-past-tense)]

[2024-07-16] ğŸ¦ Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2407.13796)][[Code](https://github.com/ltroin/Clip1)]

[2024-07-05] ğŸ¦ Jailbreak Attacks and Defenses Against Large Language Models: A Survey [[Paper](https://arxiv.org/pdf/2407.03876)]

[2024-07-04] ğŸ¦ DART: Deep Adversarial Automated Red Teaming for LLM Safety [[Paper](https://arxiv.org/pdf/2407.03876)]
 
[2024-07-02] ğŸ¦ SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack [[Paper](https://arxiv.org/pdf/2407.01902)][[Code](https://github.com/Yang-Yan-Yang-Yan/SoP)]

[2024-07-02] ğŸ¦ A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses [[Paper](https://arxiv.org/pdf/2407.02551)]

[2024-07-01] ğŸ¦ Badllama 3: removing safety finetuning from Llama 3 in minutes [[Paper](https://arxiv.org/pdf/2407.01376)]

[2024-06-28] â˜€ï¸ Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection [[Paper](https://arxiv.org/pdf/2406.19845)]

[2024-06-28] â˜€ï¸ Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation [[Paper](https://arxiv.org/pdf/2406.20053)]

[2024-06-26] â˜€ï¸ JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models [[Paper](https://arxiv.org/pdf/2407.01599)][[Code](https://chonghan-chen.com/llm-jailbreak-zoo-survey/)]

[2024-06-26] â˜€ï¸ Poisoned LangChain: Jailbreak LLMs by LangChain [[Paper](https://arxiv.org/pdf/2406.18122)][[Code](https://github.com/CAM-FSS/jailbreak-langchain)]

[2024-06-26] â˜€ï¸ WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs [[Paper](https://arxiv.org/pdf/2406.18495)][[Code](https://github.com/allenai/wildguard)]

[2024-06-26] â˜€ï¸ WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models [[Paper](https://arxiv.org/pdf/2406.18510)][[Code](https://huggingface.co/datasets/allenai/wildjailbreak)]

[2024-06-20] â˜€ï¸ Adversaries Can Misuse Combinations of Safe Models [[Paper](https://arxiv.org/pdf/2406.14595)][[Code](https://github.com/ejones313/multi-model-misuse)]

[2024-06-18] â˜€ï¸ Jailbreak Paradox: The Achilles' Heel of LLMs [[Paper](https://arxiv.org/pdf/2406.12702)]

[2024-06-17] â˜€ï¸ "Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak [[Paper](https://arxiv.org/pdf/2406.11668)][[Code](https://github.com/Meirtz/BabyBLUE-llm)]

[2024-06-17] â˜€ï¸ Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack [[Paper](https://arxiv.org/pdf/2406.11682)][[Code](https://github.com/THU-KEG/Knowledge-to-Jailbreak/)]

[2024-06-13] â˜€ï¸ Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models [[Paper](https://arxiv.org/pdf/2406.09289)][[Code](https://github.com/s-ball-10/jailbreak_dynamics)]

[2024-06-13] â˜€ï¸ StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure [[Paper](https://arxiv.org/pdf/2406.08754)]

[2024-06-13] â˜€ï¸ When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search [Paper](https://arxiv.org/pdf/2406.08705)]

[2024-06-13] â˜€ï¸ RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs [Paper](https://arxiv.org/pdf/2406.08725)]

[2024-06-13] â˜€ï¸ Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs [Paper](https://arxiv.org/pdf/2406.09324)][[Code](https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking)]

[2024-06-13] â˜€ï¸ How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States [[Paper](https://arxiv.org/pdf/2406.05644)][[Code](https://github.com/ydyjya/LLM-IHS-Explanation)]

[2024-06-11] â˜€ï¸ Merging Improves Self-Critique Against Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2406.07188)][[Code](https://github.com/vicgalle/merging-self-critique-jailbreaks)]

[2024-06-06] â˜€ï¸ AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens [[Paper](https://arxiv.org/pdf/2406.03805)]

[2024-06-03] â˜€ï¸ Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses [[Paper](https://arxiv.org/pdf/2406.01288)][[Code](https://github.com/sail-sg/I-FSJ)]

[2024-05-31] ğŸŒº Improved Techniques for Optimization-Based Jailbreaking on Large Language Models [[Paper](https://arxiv.org/pdf/2405.21018)][[Code](https://github.com/jiaxiaojunQAQ/I-GCG)]

[2024-05-30] ğŸŒº Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters [[Paper](https://arxiv.org/pdf/2405.20413)][[Code](https://github.com/Allen-piexl/llm_moderation_attack)]

[2024-05-28] ğŸŒº Are PPO-ed Language Models Hackable? [[Paper](https://arxiv.org/pdf/2406.02577)]

[2024-05-28] ğŸŒº Improved Generation of Adversarial Examples Against Safety-aligned LLMs [[Paper](https://arxiv.org/pdf/2405.20778)]

[2024-05-24] ğŸŒº Hacc-Man: An Arcade Game for Jailbreaking LLMs [[Paper](https://arxiv.org/pdf/2405.15902)]

[2024-05-21] ğŸŒº GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation [[Paper](https://arxiv.org/pdf/2405.13077)]

[2024-05-20] ğŸŒº Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation [[Paper](https://arxiv.org/pdf/2405.13068)]

[2024-05-09] ğŸŒº Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM [[Paper](https://arxiv.org/pdf/2405.05610)][[Code](https://github.com/YancyKahn/CoA)]

[2024-05-06] ğŸŒº Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent [[Paper](https://arxiv.org/pdf/2405.03654)]

[2024-04-24] ğŸŒ¸ Universal Adversarial Triggers Are Not Universal [[Paper](https://arxiv.org/pdf/2404.16020)][[Code](https://github.com/McGill-NLP/AdversarialTriggers)]

[2024-04-22] ğŸŒ¸ Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs [[Paper](https://arxiv.org/pdf/2404.14461)][[Code](https://github.com/ethz-spylab/rlhf_trojan_competition)]

[2024-04-21] ğŸŒ¸ AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs [[Paper](https://arxiv.org/pdf/2404.16873)][[Code](https://github.com/facebookresearch/advprompter)]

[2024-04-12] ğŸŒ¸ JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models [[Paper](https://arxiv.org/pdf/2404.08793)]

[2024-04-11] ğŸŒ¸ AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs LLM [[Paper](https://arxiv.org/pdf/2404.07921)][[Code](https://github.com/OSU-NLP-Group/AmpleGCG)]

[2024-04-09] ğŸŒ¸ Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs [[Paper](https://arxiv.org/pdf/2404.07242)]

[2024-04-09] ğŸŒ¸ Rethinking How to Evaluate Language Model Jailbreak [[Paper](https://arxiv.org/pdf/2404.06407v3)][[Code](https://github.com/controllability/jailbreak-evaluation)]

[2024-04-08] ğŸŒ¸ Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge [[Paper](https://arxiv.org/pdf/2404.05880)]

[2024-04-05] ğŸŒ¸ Increased LLM Vulnerabilities from Fine-tuning and Quantization [[Code](https://arxiv.org/pdf/2404.04392.pdf)]

[2024-04-02] ğŸŒ¸ Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack [[Paper](https://arxiv.org/pdf/2404.01833.pdf)]

[2024-04-02] ğŸŒ¸ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks [[Paper](https://arxiv.org/pdf/2404.02151.pdf)][[Code](https://github.com/tml-epfl/llm-adaptive-attacks)]

[2024-04-02] ğŸŒ¸ Many-shot Jailbreaking [[Paper](https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf)]

[2024-03-28] ğŸŒ± JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2404.01318.pdf)][[Code](https://github.com/JailbreakBench/jailbreakbench)]


[2024-03-19] ğŸŒ± RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content [[Paper](https://arxiv.org/pdf/2403.13031.pdf)]

[2024-03-18] ğŸŒ± EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2403.12171.pdf)][[Code](https://github.com/EasyJailbreak/EasyJailbreak)]

[2024-03-13] ğŸŒ± Tastle: Distract Large Language Models for Automatic Jailbreak Attack [[Paper](https://arxiv.org/pdf/2403.08424.pdf)]

[2024-03-12] ğŸŒ± Exploring Safety Generalization Challenges of Large Language Models via Code [[Paper](https://arxiv.org/pdf/2403.07865.pdf)]

[2024-02-28] ğŸ’• Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction [[Paper](https://arxiv.org/pdf/2402.18104.pdf)][[Code](https://sites.google.com/view/dra-jailbreak/)]

[2024-02-26] ğŸ’• CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2402.16717.pdf)][[Code](https://github.com/huizhang-L/CodeChameleon)]

[2024-02-25] ğŸ’• From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings [[Paper](https://arxiv.org/pdf/2402.16006.pdf)]

[2024-02-25] ğŸ’• DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers [[Paper](https://arxiv.org/pdf/2402.16914.pdf)][[Code](https://github.com/xirui-li/DrAttack)]


[2024-02-24] ğŸ’• PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails [[Paper](https://arxiv.org/pdf/2402.15911.pdf)]

[2024-02-23] ğŸ’• How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries [[Paper](https://arxiv.org/pdf/2402.15302.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA)]

[2024-02-21] ğŸ’• Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs [[Paper](https://arxiv.org/pdf/2402.14872.pdf)]

[2024-02-20] ğŸ’• Is the System Message Really Important to Jailbreaks in Large Language Models? [[Paper](https://arxiv.org/pdf/2402.14857.pdf)]

[2024-02-21] ğŸ’• LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper](https://arxiv.org/pdf/2402.13457.pdf)][[Code](https://sites.google.com/view/llmcomprehensive/home)]

[2024-02-21] ğŸ’• Coercing LLMs to do and reveal (almost) anything [[Paper](https://arxiv.org/pdf/2402.14020.pdf)][[Code](https://github.com/JonasGeiping/carving)]

[2024-02-19] ğŸ’• Query-Based Adversarial Prompt Generation [[Paper](https://arxiv.org/pdf/2402.12329.pdf)]

[2024-02-19] ğŸ’• ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs [[Paper](https://arxiv.org/pdf/2402.11753.pdf)][[Code](https://github.com/uw-nsl/ArtPrompt)]

[2024-02-19] ğŸ’• SPML: A DSL for Defending Language Models Against Prompt Attacks [[Paper](https://arxiv.org/pdf/2402.11755.pdf)][[Code](https://prompt-compiler.github.io/SPML/)]

[2024-02-16] ğŸ’• Jailbreaking Proprietary Large Language Models using Word Substitution Cipher [[Paper](https://arxiv.org/pdf/2402.10601.pdf)]

[2024-02-16] ğŸ’• ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages [[Paper](https://arxiv.org/pdf/2402.10753.pdf)][[Code](https://github.com/Junjie-Ye/ToolSword)]

[2024-02-15] ğŸ’• A StrongREJECT for Empty Jailbreaks [[Paper](https://arxiv.org/pdf/2402.10260.pdf)][[Code](https://github.com/alexandrasouly/strongreject)]

[2024-02-15] ğŸ’• PAL: Proxy-Guided Black-Box Attack on Large Language Models [[Paper](https://arxiv.org/pdf/2402.09674.pdf)][[Code](https://github.com/chawins/pal)]

[2024-02-14] ğŸ’• Attacking Large Language Models with Projected Gradient Descent [[Paper](https://arxiv.org/pdf/2402.09154.pdf)]

[2024-02-14] ğŸ’• Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues [[Paper](https://arxiv.org/pdf/2402.09091.pdf)]

[2024-02-13] ğŸ’• COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability [[Paper](https://arxiv.org/pdf/2402.08679.pdf)][[Code](https://github.com/Yu-Fangxu/COLD-Attack)]

[2024-02-13] ğŸ’• Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning [[Paper](https://arxiv.org/pdf/2402.08416.pdf)]

[2024-02-08] ğŸ’• Comprehensive Assessment of Jailbreak Attacks Against LLMs [[Paper](https://arxiv.org/pdf/2402.05668.pdf)]

[2024-02-06] ğŸ’• HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal [[Paper](https://arxiv.org/pdf/2402.04249.pdf)][[Code](https://github.com/centerforaisafety/HarmBench)]

[2024-02-05] ğŸ’• GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models [[Paper](https://arxiv.org/pdf/2402.03299.pdf)][[Code](https://github.com/Allen-piexl/GUARD)]

[2024-01-30] â„ï¸ A Cross-Language Investigation into Jailbreak Attacks in Large Language Models [[Paper](https://arxiv.org/pdf/2401.16765.pdf)]


[2024-01-30] â„ï¸ Weak-to-Strong Jailbreaking on Large Language Models [[Paper](https://arxiv.org/pdf/2401.17256.pdf)][[Code](https://github.com/XuandongZhao/weak-to-strong)]

[2024-01-22] â„ï¸ PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety [[Paper](https://arxiv.org/pdf/2401.11880.pdf)][[Code](https:/github.com/AI4Good24/PsySafe)]

[2024-01-19] â„ï¸ Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models [[Paper](https://arxiv.org/pdf/2401.10647.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA)]

[2024-01-18] â„ï¸ All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2401.09798.pdf)][[Code](https://github.com/kztakemoto/simbaja)]

[2024-01-17] â„ï¸ AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models [[Paper](https://arxiv.org/pdf/2401.09002.pdf)][[Code](https://github.com/BWangCN/AttackEval)]


[2024-01-16] â„ï¸ On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs [[Paper](https://openreview.net/pdf?id=H3UayAQWoE)][[Code](https://github.com/CUHK-ARISE/PsychoBench)]

[2024-01-16] â„ï¸ Understanding Hidden Context in Preference Learning: Consequences for RLHF [[Paper](https://openreview.net/pdf?id=0tWTxYYPnW)]

[2024-01-12] â„ï¸ How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs [[Paper](https://arxiv.org/pdf/2401.06373.pdf)][[Code](https://github.com/CHATS-lab/persuasive_jailbreaker)]

[2023-12-18] ğŸ„ A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models [[Paper](https://arxiv.org/pdf/2312.10982.pdf)]

[2023-12-07] ğŸ„ Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak [[Paper](https://arxiv.org/pdf/2312.04127.pdf)]

[2023-12-04] ğŸ„ Tree of Attacks: Jailbreaking Black-Box LLMs Automatically [[Paper](https://arxiv.org/pdf/2312.02119.pdf)]

[2023-12-08] ğŸ„ Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs [[Paper](https://arxiv.org/pdf/2312.04782.pdf)][[Code](https://img.shields.io/badge/CodeGen-87b800)]

[2023-11-24] ğŸ¦ƒ Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles [[Paper](https://arxiv.org/pdf/2311.14876.pdf)]

[2023-11-21] ğŸ¦ƒ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs [[Paper](https://arxiv.org/pdf/2309.11830.pdf)][[Code](https://github.com/liuchengyuan123/CPAD)]


[2023-11-14] ğŸ¦ƒ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts Can Fool Large Language Models Easily [[Paper](https://arxiv.org/pdf/2311.08268.pdf)][[Code](https://github.com/NJUNLP/ReNeLLM)]

[2023-11-10] ğŸ¦ƒ Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild [[Paper](https://arxiv.org/pdf/2311.06237.pdf)]


[2023-11-06] ğŸ¦ƒ Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation [[Paper](https://arxiv.org/pdf/2311.03348.pdf)]

[2023-11-06] ğŸ¦ƒ DeepInception: Hypnotize Large Language Model to Be Jailbreaker [[Paper](https://arxiv.org/pdf/2311.03191.pdf)][[Code](https://github.com/tmlr-group/DeepInception?tab=readme-ov-file)]

[2023-10-24] ğŸƒ Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition [[Paper](https://arxiv.org/pdf/2311.16119.pdf)]

[2023-10-23] ğŸƒ AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2310.15140.pdf)][[Code](https://github.com/rotaryhammer/code-autodan)]

[2023-10-19] ğŸƒ Attack Prompt Generation for Red Teaming and Defending Large Language Models [[Paper](https://arxiv.org/pdf/2310.12505.pdf)][[Code](https://github.com/Aatrox103/SAP)]

[2023-10-16] ğŸƒ Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attack [[Paper](https://arxiv.org/pdf/2310.10844.pdf)]

[2023-10-12] ğŸƒ Jailbreaking Black Box Large Language Models in Twenty Queries [[Paper](https://arxiv.org/pdf/2310.08419.pdf)][[Code](https://github.com/patrickrchao/JailbreakingLLMs)]

[2023-10-10] ğŸƒ Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations [[Paper](https://arxiv.org/pdf/2310.06387.pdf)]

[2023-10-10] ğŸƒ Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation [[Paper](https://arxiv.org/pdf/2310.06987.pdf)][[Code](https://github.com/Princeton-SysML/Jailbreak_LLM)]

[2023-10-03] ğŸƒ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models [[Paper](https://arxiv.org/pdf/2310.04451.pdf)][[Code](https://github.com/SheltonLiu-N/AutoDAN)]

[2023-09-19] ğŸ‚ GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts [[Paper](https://arxiv.org/pdf/2309.10253.pdf)][[Code](https://github.com/sherdencooper/GPTFuzz)]

[2023-09-11] ğŸ‚ FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models [[Paper](https://arxiv.org/pdf/2309.05274.pdf)][[Code](https://github.com/RainJamesY/FuzzLLM)]

[2023-09-04] ğŸ‚ Open Sesame! Universal Black Box Jailbreaking of Large Language Models [[Paper](https://arxiv.org/pdf/2309.01446.pdf)]

[2023-08-07] ğŸŒ´ â€œDo Anything Nowâ€: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models [[Paper](https://arxiv.org/pdf/2308.03825.pdf)][[Code](https://github.com/verazuo/jailbreak_llms)]

[2023-07-27] ğŸ¦ Universal and Transferable Adversarial Attacks on Aligned Language Models [[Paper](https://arxiv.org/pdf/2307.15043.pdf)][[Code](https://github.com/llm-attacks/llm-attacks)]

[2023-07-20] ğŸ¦ LLM Censorship: A Machine Learning Challenge Or A Computer Security Problem? [[Paper](https://arxiv.org/pdf/2307.10719.pdf)]

[2023-07-16] ğŸ¦ MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots [[Paper](https://arxiv.org/pdf/2307.08715.pdf)][[Code](https://sites.google.com/view/ndss-masterkey)]

[2023-07-05] ğŸ¦ Jailbroken: How Does LLM Safety Training Fail? [[Paper](https://arxiv.org/pdf/2307.02483.pdf)]

[2023-07-01] ğŸ¦ From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy [[Paper](https://ieeexplore.ieee.org/abstract/document/10198233)]

[2023-05-24] ğŸŒº Adversarial Demonstration Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2305.14950.pdf)]

[2023-05-24] ğŸŒº Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks [[Paper](https://arxiv.org/pdf/2305.14965.pdf)][[Code](https://github.com/AetherPrior/TrickLLM)]

[2023-05-23] ğŸŒº Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [[Paper](https://arxiv.org/pdf/2305.13860.pdf)][[Code](https://sites.google.com/view/llm-jailbreak-study)]

[2023-04-11] ğŸŒ¸ Multi-step Jailbreaking Privacy Attacks on ChatGPT [[Paper](https://arxiv.org/pdf/2304.05197.pdf)][[Code](https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak)]

[2023-03-08] ğŸŒ± Automatically Auditing Large Language Models via Discrete Optimization [[Paper](https://proceedings.mlr.press/v202/jones23a/jones23a.pdf)][[Code](https://github.com/ejones313/auditing-llms)]

[2023-02-11] ğŸ’• Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks [[Paper](https://arxiv.org/pdf/2302.05733.pdf)]

