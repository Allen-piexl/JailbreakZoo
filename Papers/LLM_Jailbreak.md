# Timeline üöÄ 
(Jan:‚ùÑÔ∏è, Feb:üíï, Mar:üå±, Apr:üå∏, May:üå∫, Jun:‚òÄÔ∏è, Jul:üç¶, Aug:üå¥, Sep:üçÇ, Oct:üéÉ, Nov:ü¶É, Dec:üéÑ)

[2024-04-02] üå∏ Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack [[Paper](https://arxiv.org/pdf/2404.01833.pdf)]

[2024-04-02] üå∏ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks [[Paper](https://arxiv.org/pdf/2404.02151.pdf)][[Code](https://github.com/tml-epfl/llm-adaptive-attacks)]

[2024-04-02] üå∏ Many-shot Jailbreaking [[Paper](https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf)]

[2024-03-28] üå± JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2404.01318.pdf)][[Code](https://github.com/JailbreakBench/jailbreakbench)]


[2024-03-19] üå± RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content[[Paper](https://arxiv.org/pdf/2403.13031.pdf)]![Gradient-based](https://img.shields.io/badge/-Gradient--based-blue) ![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-03-18] üå± EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2403.12171.pdf)][[Code](https://github.com/EasyJailbreak/EasyJailbreak)]![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-03-13] üå± Tastle: Distract Large Language Models for Automatic Jailbreak Attack [[Paper](https://arxiv.org/pdf/2403.08424.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-03-12] üå± Exploring Safety Generalization Challenges of Large Language Models via Code [[Paper](https://arxiv.org/pdf/2403.07865.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-02-28] üíï Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction [[Paper](https://arxiv.org/pdf/2402.18104.pdf)][[Code](https://sites.google.com/view/dra-jailbreak/)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-02-26] üíï CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/pdf/2402.16717.pdf)][[Code](https://github.com/huizhang-L/CodeChameleon)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-02-25] üíï From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings [[Paper](https://arxiv.org/pdf/2402.16006.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-02-25] üíï DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers [[Paper](https://arxiv.org/pdf/2402.16914.pdf)][[Code](https://github.com/xirui-li/DrAttack)]![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)


[2024-02-24] üíï PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails [[Paper](https://arxiv.org/pdf/2402.15911.pdf)]![Prompt-based](https://img.shields.io/badge/-Prompt--based-red)![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-23] üíï How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries [[Paper](https://arxiv.org/pdf/2402.15302.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA)] ![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-21] üíï Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs [[Paper](https://arxiv.org/pdf/2402.14872.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2024-02-20] üíï Is the System Message Really Important to Jailbreaks in Large Language Models? [[Paper](https://arxiv.org/pdf/2402.14857.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-21] üíï LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper](https://arxiv.org/pdf/2402.13457.pdf)][[Code](https://sites.google.com/view/llmcomprehensive/home)]![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-21] üíï Coercing LLMs to do and reveal (almost) anything [[Paper](https://arxiv.org/pdf/2402.14020.pdf)][[Code](https://github.com/JonasGeiping/carving)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-19] üíï Query-Based Adversarial Prompt Generation [[Paper](https://arxiv.org/pdf/2402.12329.pdf)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)

[2024-02-19] üíï ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs [[Paper](https://arxiv.org/pdf/2402.11753.pdf)][[Code](https://github.com/uw-nsl/ArtPrompt)] ![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-19] üíï SPML: A DSL for Defending Language Models Against Prompt Attacks [[Paper](https://arxiv.org/pdf/2402.11755.pdf)][[Code](https://prompt-compiler.github.io/SPML/)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-16] üíï Jailbreaking Proprietary Large Language Models using Word Substitution Cipher [[Paper](https://arxiv.org/pdf/2402.10601.pdf)]![Decomposition-based](https://img.shields.io/badge/-Decomposition--based-orange)

[2024-02-16] üíï ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages [[Paper](https://arxiv.org/pdf/2402.10753.pdf)][[Code](https://github.com/Junjie-Ye/ToolSword)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey) ![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-15] üíï A StrongREJECT for Empty Jailbreaks [[Paper](https://arxiv.org/pdf/2402.10260.pdf)][[Code](https://github.com/alexandrasouly/strongreject)]![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-15] üíï PAL: Proxy-Guided Black-Box Attack on Large Language Models [[Paper](https://arxiv.org/pdf/2402.09674.pdf)][[Code](https://github.com/chawins/pal)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2024-02-14] üíï Attacking Large Language Models with Projected Gradient Descent [[Paper](https://arxiv.org/pdf/2402.09154.pdf)]![Gradient-based](https://img.shields.io/badge/-Gradient--based-blue) ![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2024-02-14] üíï Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues [[Paper](https://arxiv.org/pdf/2402.09091.pdf)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)

[2024-02-13] üíï COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability [[Paper](https://arxiv.org/pdf/2402.08679.pdf)][[Code](https://github.com/Yu-Fangxu/COLD-Attack)]![Gradient-based](https://img.shields.io/badge/-Gradient--based-blue) ![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-13] üíï Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning [[Paper](https://arxiv.org/pdf/2402.08416.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2024-02-08] üíï Comprehensive Assessment of Jailbreak Attacks Against LLMs [[Paper](https://arxiv.org/pdf/2402.05668.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-06] üíï HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal [[Paper](https://arxiv.org/pdf/2402.04249.pdf)][[Code](https://github.com/centerforaisafety/HarmBench)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-02-05] üíï GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models [[Paper](https://arxiv.org/pdf/2402.03299.pdf)][[Code](https://github.com/Allen-piexl/GUARD)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2024-01-30] ‚ùÑÔ∏è A Cross-Language Investigation into Jailbreak Attacks in Large Language Models [[Paper](https://arxiv.org/pdf/2401.16765.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2024-01-30] ‚ùÑÔ∏è Weak-to-Strong Jailbreaking on Large Language Models [[Paper](https://arxiv.org/pdf/2401.17256.pdf)][[Code](https://github.com/XuandongZhao/weak-to-strong)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-01-22] ‚ùÑÔ∏è PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety [[Paper](https://arxiv.org/pdf/2401.11880.pdf)][[Code](https:/github.com/AI4Good24/PsySafe)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-01-19] ‚ùÑÔ∏è Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models [[Paper](https://arxiv.org/pdf/2401.10647.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2024-01-18] ‚ùÑÔ∏è All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2401.09798.pdf)][[Code](https://github.com/kztakemoto/simbaja)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)

[2024-01-17] ‚ùÑÔ∏è AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models [[Paper](https://arxiv.org/pdf/2401.09002.pdf)][[Code](https://github.com/BWangCN/AttackEval)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2024-01-16] ‚ùÑÔ∏è On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs [[Paper](https://openreview.net/pdf?id=H3UayAQWoE)][[Code](https://github.com/CUHK-ARISE/PsychoBench)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)

[2024-01-16] ‚ùÑÔ∏è Understanding Hidden Context in Preference Learning: Consequences for RLHF [[Paper](https://openreview.net/pdf?id=0tWTxYYPnW)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2024-01-12] ‚ùÑÔ∏è How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs [[Paper](https://arxiv.org/pdf/2401.06373.pdf)][[Code](https://github.com/CHATS-lab/persuasive_jailbreaker)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2023-12-18] üéÑ A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models [[Paper](https://arxiv.org/pdf/2312.10982.pdf)]![Survey](https://img.shields.io/badge/-Survey-ff69b4)

[2023-12-07] üéÑ Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak [[Paper](https://arxiv.org/pdf/2312.04127.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2023-12-04] üéÑ Tree of Attacks: Jailbreaking Black-Box LLMs Automatically [[Paper](https://arxiv.org/pdf/2312.02119.pdf)]![Query-based](https://img.shields.io/badge/-Query--based-lightgrey)

[2023-12-08] üéÑ Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs [[Paper](https://arxiv.org/pdf/2312.04782.pdf)][[Code](https://img.shields.io/badge/CodeGen-87b800)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2023-11-24] ü¶É Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles [[Paper](https://arxiv.org/pdf/2311.14876.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2023-11-21] ü¶É Goal-Oriented Prompt Attack and Safety Evaluation for LLMs [[Paper](https://arxiv.org/pdf/2309.11830.pdf)][[Code](https://github.com/liuchengyuan123/CPAD)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2023-11-14] ü¶É A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts Can Fool Large Language Models Easily [[Paper](https://arxiv.org/pdf/2311.08268.pdf)][[Code](https://github.com/NJUNLP/ReNeLLM)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)

[2023-11-10] ü¶É Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild [[Paper](https://arxiv.org/pdf/2311.06237.pdf)]![Optimization-based](https://img.shields.io/badge/-Optimization--based-brightgreen)![Evaluation](https://img.shields.io/badge/-Evaluation-yellowgreen)


[2023-11-06] ü¶É Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation [[Paper](https://arxiv.org/pdf/2311.03348.pdf)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-11-06] ü¶É DeepInception: Hypnotize Large Language Model to Be Jailbreaker [[Paper](https://arxiv.org/pdf/2311.03191.pdf)][[Code](https://github.com/tmlr-group/DeepInception?tab=readme-ov-file)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-10-24] üéÉ Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition [[Paper](https://arxiv.org/pdf/2311.16119.pdf)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-10-23] üéÉ AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2310.15140.pdf)][[Code](https://github.com/rotaryhammer/code-autodan)]

[2023-10-19] üéÉ Attack Prompt Generation for Red Teaming and Defending Large Language Models [[Paper](https://arxiv.org/pdf/2310.12505.pdf)][[Code](https://github.com/Aatrox103/SAP)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-10-16] üéÉ Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attack [[Paper](https://arxiv.org/pdf/2310.10844.pdf)]

[2023-10-12] üéÉ Jailbreaking Black Box Large Language Models in Twenty Queries [[Paper](https://arxiv.org/pdf/2310.08419.pdf)][[Code](https://github.com/patrickrchao/JailbreakingLLMs)]

[2023-10-10] üéÉ Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations [[Paper](https://arxiv.org/pdf/2310.06387.pdf)]

[2023-10-10] üéÉ Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation [[Paper](https://arxiv.org/pdf/2310.06987.pdf)][[Code](https://github.com/Princeton-SysML/Jailbreak_LLM)]

[2023-10-03] üéÉ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models [[Paper](https://arxiv.org/pdf/2310.04451.pdf)][[Code](https://github.com/SheltonLiu-N/AutoDAN)]

[2023-09-19] üçÇ GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts [[Paper](https://arxiv.org/pdf/2309.10253.pdf)][[Code](https://github.com/sherdencooper/GPTFuzz)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-09-11] üçÇ FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models [[Paper](https://arxiv.org/pdf/2309.05274.pdf)][[Code](https://github.com/RainJamesY/FuzzLLM)]

[2023-09-04] üçÇ Open Sesame! Universal Black Box Jailbreaking of Large Language Models [[Paper](https://arxiv.org/pdf/2309.01446.pdf)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-08-07] üå¥ ‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models [[Paper](https://arxiv.org/pdf/2308.03825.pdf)][[Code](https://github.com/verazuo/jailbreak_llms)]

[2023-07-27] üç¶ Universal and Transferable Adversarial Attacks on Aligned Language Models [[Paper](https://arxiv.org/pdf/2307.15043.pdf)][[Code](https://github.com/llm-attacks/llm-attacks)]![Gradient-based](https://img.shields.io/badge/-Gradient--based-blue) ![Whitebox](https://img.shields.io/badge/-Whitebox-white)

[2023-07-20] üç¶ LLM Censorship: A Machine Learning Challenge Or A Computer Security Problem? [[Paper](https://arxiv.org/pdf/2307.10719.pdf)]

[2023-07-16] üç¶ MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots [[Paper](https://arxiv.org/pdf/2307.08715.pdf)][[Code](https://sites.google.com/view/ndss-masterkey)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-07-05] üç¶ Jailbroken: How Does LLM Safety Training Fail? [[Paper](https://arxiv.org/pdf/2307.02483.pdf)]

[2023-07-01] üç¶ From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy [[Paper](https://ieeexplore.ieee.org/abstract/document/10198233)]

[2023-05-24] üå∫ Adversarial Demonstration Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2305.14950.pdf)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-05-24] üå∫ Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks [[Paper](https://arxiv.org/pdf/2305.14965.pdf)][[Code](https://github.com/AetherPrior/TrickLLM)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-05-23] üå∫ Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [[Paper](https://arxiv.org/pdf/2305.13860.pdf)][[Code](https://sites.google.com/view/llm-jailbreak-study)]

[2023-04-11] üå∏ Multi-step Jailbreaking Privacy Attacks on ChatGPT [[Paper](https://arxiv.org/pdf/2304.05197.pdf)][[Code](https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

[2023-03-08] üå± Automatically Auditing Large Language Models via Discrete Optimization [[Paper](https://proceedings.mlr.press/v202/jones23a/jones23a.pdf)][[Code](https://github.com/ejones313/auditing-llms)]

[2023-02-11] üíï Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks [[Paper](https://arxiv.org/pdf/2302.05733.pdf)]![Blackbox](https://img.shields.io/badge/-Blackbox-black)

