# Timeline ğŸš€ 
(Jan:â„ï¸, Feb:ğŸ’•, Mar:ğŸŒ±, Apr:ğŸŒ¸, May:ğŸŒº, Jun:â˜€ï¸, Jul:ğŸ¦, Aug:ğŸŒ´, Sep:ğŸ‚, Oct:ğŸƒ, Nov:ğŸ¦ƒ, Dec:ğŸ„)

[2024-03-12] ğŸŒ± Exploring Safety Generalization Challenges of Large Language Models via Code [[Paper](https://arxiv.org/pdf/2403.07865.pdf)]

[2024-02-28] ğŸ’• Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction [[Paper](https://arxiv.org/pdf/2402.18104.pdf)]

[2024-02-26] ğŸ’• CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models [[Paper](https://arxiv.org/abs/2402.16717.pdf)][[Code](https://github.com/huizhang-L/CodeChameleon)]

[2024-02-25] ğŸ’• DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers [[Paper](https://arxiv.org/pdf/2402.16914.pdf)][[Code](https://github.com/xirui-li/DrAttack)]

[2024-02-24] ğŸ’• PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails [[Paper](https://arxiv.org/pdf/2402.15911.pdf)]

[2024-02-05] ğŸ’• GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models [[Paper](https://arxiv.org/abs/2402.03299.pdf)][[Code](https://github.com/Allen-piexl/GUARD)]

[2024-01-30] â„ï¸ A Cross-Language Investigation into Jailbreak Attacks in Large Language Models [[Paper](https://arxiv.org/pdf/2401.16765.pdf)]

[2024-01-30] â„ï¸ Weak-to-Strong Jailbreaking on Large Language Models [[Paper](https://arxiv.org/abs/2401.17256.pdf)][[Code](https://github.com/XuandongZhao/weak-to-strong)]

[2024-01-22] â„ï¸ PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety [[Paper](https://arxiv.org/abs/2401.11880.pdf)][[Code](https:/github.com/AI4Good24/PsySafe)]

[2024-01-19] â„ï¸ Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models [[Paper](https://arxiv.org/abs/2401.10647.pdf)][[Code](https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA)]

[2024-01-18] â„ï¸ All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks [[Paper](https://arxiv.org/abs/2401.09798.pdf)][[Code](https://github.com/kztakemoto/simbaja)]

[2024-01-17] â„ï¸ AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models [[Paper](https://arxiv.org/abs/2401.09002)][[Code](https://github.com/BWangCN/AttackEval)]

[2024-01-12] â„ï¸ How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs [[Paper](https://arxiv.org/pdf/2401.06373.pdf)][[Code](https://github.com/CHATS-lab/persuasive_jailbreaker)]

[2023-05-24] ğŸŒº Adversarial Demonstration Attacks on Large Language Models [[Paper](https://arxiv.org/pdf/2305.14950.pdf)]

[2023-05-24] ğŸŒº Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks [[Paper](https://arxiv.org/pdf/2305.14965.pdf)][[Code](https://github.com/AetherPrior/TrickLLM)]

[2023-05-23] ğŸŒº Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [[Paper](https://arxiv.org/pdf/2305.13860.pdf)][[Code](https://sites.google.com/view/llm-jailbreak-study)]

[2023-04-11] ğŸŒ¸ Multi-step Jailbreaking Privacy Attacks on ChatGPT [[Paper](https://arxiv.org/pdf/2304.05197.pdf)][[Code](https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak)]

[2023-03-08] ğŸŒ± Automatically Auditing Large Language Models via Discrete Optimization [[Paper](https://proceedings.mlr.press/v202/jones23a/jones23a.pdf)][[Code](https://github.com/ejones313/auditing-llms)]
