# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-06-06] ☀️ Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt [[Paper](https://arxiv.org/pdf/2406.04031)][[Code](https://github.com/NY1024/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt)]

[2024-05-29] 🌺 Voice Jailbreak Attacks Against GPT-4o [[Paper](https://arxiv.org/pdf/2405.19103)][[Code](https://github.com/TrustAIRLab/VoiceJailbreakAttack)]

[2024-05-27] 🌺 Cross-Modal Safety Alignment: Is textual unlearning all you need? [[Paper](https://arxiv.org/pdf/2406.02575)]

[2024-05-26] 🌺 Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models VLM [[Paper](https://arxiv.org/pdf/2405.20775)][[Code](https://github.com/dirtycomputer/O2M_attack)]

[2024-05-25] 🌺 Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte VLM [[Paper](https://arxiv.org/pdf/2405.20773)]


[2024-04-04] 🌸 Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? [[Paper](https://arxiv.org/pdf/2404.03411.pdf)][[Code](https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/)]

[2024-04-03] 🌸 JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2404.03027.pdf)][[Code](https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k)]

[2024-04-02] 🌸 Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models [[Paper](https://arxiv.org/pdf/2404.02928.pdf)]

[2024-03-14] 🌱 Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models [[Paper](https://arxiv.org/pdf/2403.09513.pdf)]

[2024-02-13] 💕 Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast [[Paper](https://arxiv.org/pdf/2402.08567.pdf)][[Code](https://sail-sg.github.io/Agent-Smith/)]

[2024-02-04] 💕 Jailbreaking Attack against Multimodal Large Language Model [[Paper](https://arxiv.org/pdf/2402.02309.pdf)][[Code](https://github.com/abc03570128/Jailbreaking-Attack-against-Multimodal-Large-Language-Model)]

[2024-01-16] ❄️ Multilingual Jailbreak Challenges in Large Language Models [[Paper](https://openreview.net/pdf?id=plmBsXHxgR)]

[2023-12-21] 🎄 Adversarial Attacks on GPT-4 via Simple Random Search [[Paper](https://www.andriushchenko.me/gpt4adv.pdf)][[Code](https://github.com/max-andr/adversarial-random-search-gpt4)]

[2023-11-15] 🦃 Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts [[Paper](https://arxiv.org/pdf/2311.09127.pdf)]

[2023-11-09] 🦃 FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts [[Paper](https://arxiv.org/pdf/2311.05608.pdf)][[Code](https://github.com/ThuCCSLab/FigStep)]

[2023-10-03] 🎃 Low-Resource Languages Jailbreak GPT-4 [[Paper](https://arxiv.org/pdf/2310.02446.pdf)]

[2023-09-25] 🍂 SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution [[Paper](https://arxiv.org/pdf/2309.14122.pdf)][[Code](https://github.com/Zjm1900/SurrogatePrompt)]

[2023-08-12] 🌴 GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher [[Paper](https://arxiv.org/pdf/2308.06463.pdf)][[Code](https://github.com/RobustNLP/CipherChat)]

[2023-06-22] ☀️ Visual Adversarial Examples Jailbreak Aligned Large Language Models [[Paper](https://arxiv.org/pdf/2306.13213.pdf)][[Code](https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models)]

[2023-05-20] 🦃 SneakyPrompt: Jailbreaking Text-to-image Generative Models [[Paper](https://arxiv.org/pdf/2305.12082.pdf)][[Code](https://github.com/Yuchen413/text2image_safety)]
