# Timeline ğŸš€ 
(Jan:â„ï¸, Feb:ğŸ’•, Mar:ğŸŒ±, Apr:ğŸŒ¸, May:ğŸŒº, Jun:â˜€ï¸, Jul:ğŸ¦, Aug:ğŸŒ´, Sep:ğŸ‚, Oct:ğŸƒ, Nov:ğŸ¦ƒ, Dec:ğŸ„)

[2024-03-14] ğŸŒ± Images are Achillesâ€™ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models [[Paper](https://arxiv.org/pdf/2403.09792.pdf)]

[2024-02-13] ğŸ’• Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast [[Paper](https://arxiv.org/pdf/2402.08567.pdf)][[Code](https://sail-sg.github.io/Agent-Smith/)]

[2024-02-04] ğŸ’• Jailbreaking Attack against Multimodal Large Language Model [[Paper](https://arxiv.org/pdf/2402.02309.pdf)][[Code](https://github.com/abc03570128/Jailbreaking-Attack-against-Multimodal-Large-Language-Model)]

[2024-02-03] ğŸ’• Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models [[Paper](https://arxiv.org/pdf/2402.02207.pdf)][[Code](https://github.com/ys-zong/VLGuard)]

[2024-01-16] â„ï¸ Multilingual Jailbreak Challenges in Large Language Models [[Paper](https://openreview.net/pdf?id=plmBsXHxgR)]

[2023-12-21] ğŸ„ Adversarial Attacks on GPT-4 via Simple Random Search [[Paper](https://www.andriushchenko.me/gpt4adv.pdf)][[Code](https://github.com/max-andr/adversarial-random-search-gpt4)]

[2023-12-17] ğŸ„ A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection [[Paper](https://arxiv.org/pdf/2312.10766.pdf)][[Code](https://github.com/shiningrain/JailGuard)]

[2023-11-29] ğŸ¦ƒ MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models [[Paper](https://arxiv.org/pdf/2311.17600.pdf)]

[2023-11-15] ğŸ¦ƒ Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts [[Paper](https://arxiv.org/pdf/2311.09127.pdf)]

[2023-10-03] ğŸƒ Low-Resource Languages Jailbreak GPT-4 [[Paper](https://arxiv.org/pdf/2310.02446.pdf)]

[2023-09-25] ğŸ‚ SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution [[Paper](https://arxiv.org/pdf/2309.14122.pdf)][[Code](https://github.com/Zjm1900/SurrogatePrompt)]

[2023-08-12] ğŸŒ´ GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher [[Paper](https://arxiv.org/pdf/2308.06463.pdf)][[Code](https://github.com/RobustNLP/CipherChat)]

[2023-05-20] ğŸ¦ƒ SneakyPrompt: Jailbreaking Text-to-image Generative Models [[Paper](https://arxiv.org/pdf/2305.12082.pdf)][[Code](https://github.com/Yuchen413/text2image_safety)]
