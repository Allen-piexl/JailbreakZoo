# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-03-02] 🌱 AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2403.04783)][[Code](https://github.com/XHMY/AutoDefense)]

[2024-03-01] 🌱 Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes [[Paper](https://arxiv.org/pdf/2403.00867.pdf)][[Code](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense)]

[2024-02-25] 💕 Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing [[Paper](https://arxiv.org/pdf/2402.16192.pdf)][[Code](https://github.com/UCSB-NLP-Chang/SemanticSmooth)]

[2024-02-24] 💕 LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper [[Paper](https://arxiv.org/pdf/2402.15727.pdf)]

[2024-02-23] 💕 Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement [[Paper](https://arxiv.org/pdf/2402.15180.pdf)][[Code](https://anonymous.4open.science/r/refine-a-broken-4E03/)]

[2024-02-22] 💕 Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment [[Paper](https://arxiv.org/pdf/2402.14968.pdf)]

[2024-02-21] 💕 LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper](https://arxiv.org/pdf/2402.13457.pdf)][[Code](https://sites.google.com/view/llmcomprehensive/home)]

[2024-02-21] 💕 GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis [[Paper](https://arxiv.org/pdf/2402.13494.pdf)][[Code](https://github.com/xyq7/GradSafe)]

[2024-02-14] 💕 SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding [[Paper](https://arxiv.org/pdf/2402.08983.pdf)][[Code](https://github.com/uw-nsl/SafeDecoding)]

[2024-01-31] ❄️ Prompt-Driven LLM Safeguarding via Directed Representation Optimization [[Paper](https://arxiv.org/pdf/2401.18018.pdf)][[Code](https://github.com/chujiezheng/LLM-Safeguard)]

[2024-01-30] ❄️ Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [[Paper](https://arxiv.org/pdf/2401.17263.pdf)][[Code](https://github.com/andyz245/rpo)]

[2024-01-19] ❄️ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning [[Paper](https://arxiv.org/pdf/2401.10862.pdf)][[Code](https://github.com/CrystalEye42/eval-safety)]

[2024-01-12] ❄️ Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender [[Paper](https://arxiv.org/pdf/2401.06561.pdf)][[Code](https://github.com/alphadl/SafeLLM_with_IntentionAnalysis)]

[2023-12-12] 🎄 Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack [[Paper](https://arxiv.org/pdf/2312.06924.pdf)][[Code](https://github.com/FYYFU/SafetyAlignNLP)]

[2023-11-20] 🦃 Evil Geniuses: Delving into the Safety of LLM-based Agents [[Paper](https://arxiv.org/pdf/2311.11855.pdf)][[Code](https://github.com/evil-genius-project/evil-genius)]

[2023-11-13] 🦃 MART: Improving LLM Safety with Multi-round Automatic Red-Teaming [[Paper](https://arxiv.org/pdf/2311.07689.pdf)]

