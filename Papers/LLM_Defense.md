# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-03-02] 🌱 AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks [[Paper](https://arxiv.org/abs/2403.04783)][[Code](https://github.com/XHMY/AutoDefense)]

[2024-03-01] 🌱 Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes [[Paper](https://arxiv.org/pdf/2403.00867.pdf)][[Code](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense)]

[2024-02-25] 💕 Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing [[Paper](https://arxiv.org/pdf/2402.16192.pdf)][[Code](https://github.com/UCSB-NLP-Chang/SemanticSmooth)]

[2024-01-30] ❄️ Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [[Paper](https://arxiv.org/abs/2401.17263.pdf)][[Code](https://github.com/andyz245/rpo)]

[2024-02-24] 💕 LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper [[Paper](https://arxiv.org/pdf/2402.15727.pdf)]

[2024-01-19] ❄️ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning [[Paper](https://arxiv.org/abs/2401.10862.pdf)][[Code](https://github.com/CrystalEye42/eval-safety)]

[2024-01-12] ❄️ Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender [[Paper](https://arxiv.org/abs/2401.06561.pdf)][[Code](https://github.com/alphadl/SafeLLM_with_IntentionAnalysis)]
