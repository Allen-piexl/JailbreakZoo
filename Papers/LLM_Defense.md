# Timeline 🚀 
(Jan:❄️, Feb:💕, Mar:🌱, Apr:🌸, May:🌺, Jun:☀️, Jul:🍦, Aug:🌴, Sep:🍂, Oct:🎃, Nov:🦃, Dec:🎄)

[2024-04-09] 🌸 AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts [[Paper](https://arxiv.org/pdf/2404.05993.pdf)]

[2024-04-08] 🌸 Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge [[Paper](https://arxiv.org/pdf/2404.05880.pdf)][[Code](https://anonymous.4open.science/r/Eraser-537E/)]

[2024-03-21] 🌱 Detoxifying Large Language Models via Knowledge Editing [[Paper]](https://arxiv.org/pdf/2403.14472.pdf) [[Code]](https://github.com/zjunlp/EasyEdit)

[2024-03-20] 🌱 Jailbreaking is Best Solved by Definition [[Paper]](https://arxiv.org/pdf/2403.14725.pdf) [[Code]](https://github.com/kothasuhas/purple-problem)

[2024-03-15] 🌱 Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework [[Paper]](https://arxiv.org/pdf/2312.00029.pdf) [[Code]](https://github.com/matthew-pisano/Bergeron)

[2024-03-04] 🌱 On Prompt-Driven Safeguarding for Large Language Models [[Paper]](https://arxiv.org/pdf/2401.18018.pdf) [[Code]](https://github.com/chujiezheng/LLM-Safeguard)

[2024-03-02] 🌱 AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks [[Paper]](https://arxiv.org/pdf/2403.04783) [[Code]](https://github.com/XHMY/AutoDefense)

[2024-03-01] 🌱 Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes [[Paper]](https://arxiv.org/pdf/2403.00867.pdf) [[Code]](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense)

[2024-02-25] 💕 Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing [[Paper]](https://arxiv.org/pdf/2402.16192.pdf) [[Code]](https://github.com/UCSB-NLP-Chang/SemanticSmooth)

[2024-02-23] 💕 Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement [[Paper]](https://arxiv.org/pdf/2402.15180.pdf) [[Code]](https://anonymous.4open.science/r/refine-a-broken-4E03/t)

[2024-02-22] 💕 Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment [[Paper](https://arxiv.org/pdf/2402.14968.pdf)]

[2024-02-21] 💕 LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper]](https://arxiv.org/pdf/2402.13457.pdf) [[Code]](https://sites.google.com/view/llmcomprehensive/home)

[2024-02-21] 💕 GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis [[Paper]](https://arxiv.org/pdf/2402.13494.pdf) [[Code]](https://github.com/xyq7/GradSafe)

[2024-02-19] 💕 Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning [[Paper]](https://arxiv.org/pdf/2402.06255.pdf)

[2024-02-14] 💕 SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding [[Paper]](https://arxiv.org/pdf/2402.08983.pdf) [[Code]](https://github.com/uw-nsl/SafeDecoding)

[2024-01-30] ❄️ Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [[Paper]](https://arxiv.org/pdf/2401.17263.pdf) [[Code]](https://github.com/andyz245/rpo)

[2024-01-19] ❄️ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning [[Paper]](https://arxiv.org/pdf/2401.10862.pdf) [[Code]](https://github.com/CrystalEye42/eval-safety)

[2024-01-12] ❄️ Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender [[Paper]](https://arxiv.org/pdf/2401.06561.pdf) [[Code]](https://github.com/alphadl/SafeLLM_with_IntentionAnalysis)

[2023-12-30] 🎄The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness [[Paper]](https://arxiv.org/pdf/2401.00287.pdf)

[2023-12-29] 🎄 Jatmo: Prompt Injection Defense by Task-Specific Finetuning [[Paper]](https://arxiv.org/pdf/2312.17673.pdf)

[2023-12-12] 🎄Defending Chatgpt against jailbreak attack via self-reminders [[Paper]](https://www.nature.com/articles/s42256-023-00765-8)

[2023-12-12] 🎄 Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack [[Paper]](https://arxiv.org/pdf/2312.06924.pdf) [[Code]](https://github.com/FYYFU/SafetyAlignNLP)

[2023-11-15] 🦃Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization[[Paper]](https://arxiv.org/pdf/2311.09096.pdf)

[2023-11-13] 🦃 MART: Improving LLM Safety with Multi-round Automatic Red-Teaming [[Paper](https://arxiv.org/pdf/2311.07689.pdf)]

[2023-10-10] 🎃 Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations [[Paper]](https://arxiv.org/pdf/2310.06387.pdf)

[2023-10-09] 🎃 SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese [[Paper]](https://arxiv.org/pdf/2310.05818.pdf) [[Code]](https://www.cluebenchmarks.com/)

[2023-10-05] 🎃 SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks [[Paper]](https://arxiv.org/pdf/2310.03684.pdf) [[Code]](https://github.com/arobey1/smooth-llm)

[2023-09-18] 🍂 Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM [[Paper]](https://arxiv.org/pdf/2309.14348.pdf) [[Code]](https://github.com/AAAAAAsuka/llm_defends)

[2023-09-13] 🍂 RAIN: Your Language Models Can Align Themselves without Finetuning [[Paper]](https://arxiv.org/pdf/2309.07124.pdf) [[Code]](https://github.com/SafeAILab/RAIN)

[2023-09-06] 🍂 Certifying LLM Safety against Adversarial Prompting [[Paper]](https://arxiv.org/pdf/2309.02705.pdf) [[Code]](https://github.com/aounon/certified-llm-safety)

[2023-09-01] 🍂 Baseline Defenses for Adversarial Attacks Against Aligned Language Models [[Paper]](https://arxiv.org/pdf/2309.00614.pdf) [[Code]](https://github.com/neelsjain/baseline-defenses)

[2023-08-27] 🌴 Detecting Language Model Attacks with Perplexity [[Paper](https://arxiv.org/pdf/2308.14132.pdf)]

[2023-08-18] 🌴 Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment [[Paper]](https://arxiv.org/pdf/2308.09662.pdf) [[Code]](https://github.com/declare-lab/red-instruct)

[2023-08-14] 🌴 LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked [[Paper]](https://arxiv.org/pdf/2308.07308.pdf)

[2023-08-02] 🌴 XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models [[Paper]](https://arxiv.org/pdf/2308.01263.pdf) [[Code]](https://github.com/paul-rottger/exaggerated-safety)

[2023-07-17] 🍦 Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models [[Paper]](https://arxiv.org/pdf/2307.08487.pdf) [[Code]](https://github.com/qiuhuachuan/latent-jailbreak)
