# Timeline ğŸš€ 
(Jan:â„ï¸, Feb:ğŸ’•, Mar:ğŸŒ±, Apr:ğŸŒ¸, May:ğŸŒº, Jun:â˜€ï¸, Jul:ğŸ¦, Aug:ğŸŒ´, Sep:ğŸ‚, Oct:ğŸƒ, Nov:ğŸ¦ƒ, Dec:ğŸ„)

[2024-04-09] ğŸŒ¸ AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts [[Paper](https://arxiv.org/pdf/2404.05993.pdf)]

[2024-04-08] ğŸŒ¸ Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge [[Paper](https://arxiv.org/pdf/2404.05880.pdf)][[Code](https://anonymous.4open.science/r/Eraser-537E/)]

[2024-03-21] ğŸŒ± Detoxifying Large Language Models via Knowledge Editing [[Paper]](https://arxiv.org/pdf/2403.14472.pdf) [[Code]](https://github.com/zjunlp/EasyEdit)

[2024-03-20] ğŸŒ± Jailbreaking is Best Solved by Definition [[Paper]](https://arxiv.org/pdf/2403.14725.pdf) [[Code]](https://github.com/kothasuhas/purple-problem)

[2024-03-15] ğŸŒ± Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework [[Paper]](https://arxiv.org/pdf/2312.00029.pdf) [[Code]](https://github.com/matthew-pisano/Bergeron)

[2024-03-04] ğŸŒ± On Prompt-Driven Safeguarding for Large Language Models [[Paper]](https://arxiv.org/pdf/2401.18018.pdf) [[Code]](https://github.com/chujiezheng/LLM-Safeguard)

[2024-03-02] ğŸŒ± AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks [[Paper]](https://arxiv.org/pdf/2403.04783) [[Code]](https://github.com/XHMY/AutoDefense)

[2024-03-01] ğŸŒ± Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes [[Paper]](https://arxiv.org/pdf/2403.00867.pdf) [[Code]](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense)

[2024-02-25] ğŸ’• Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing [[Paper]](https://arxiv.org/pdf/2402.16192.pdf) [[Code]](https://github.com/UCSB-NLP-Chang/SemanticSmooth)

[2024-02-23] ğŸ’• Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement [[Paper]](https://arxiv.org/pdf/2402.15180.pdf) [[Code]](https://anonymous.4open.science/r/refine-a-broken-4E03/t)

[2024-02-22] ğŸ’• Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment [[Paper](https://arxiv.org/pdf/2402.14968.pdf)]

[2024-02-21] ğŸ’• LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper]](https://arxiv.org/pdf/2402.13457.pdf) [[Code]](https://sites.google.com/view/llmcomprehensive/home)

[2024-02-21] ğŸ’• GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis [[Paper]](https://arxiv.org/pdf/2402.13494.pdf) [[Code]](https://github.com/xyq7/GradSafe)

[2024-02-19] ğŸ’• Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning [[Paper]](https://arxiv.org/pdf/2402.06255.pdf)

[2024-02-14] ğŸ’• SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding [[Paper]](https://arxiv.org/pdf/2402.08983.pdf) [[Code]](https://github.com/uw-nsl/SafeDecoding)

[2024-01-30] â„ï¸ Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [[Paper]](https://arxiv.org/pdf/2401.17263.pdf) [[Code]](https://github.com/andyz245/rpo)

[2024-01-19] â„ï¸ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning [[Paper]](https://arxiv.org/pdf/2401.10862.pdf) [[Code]](https://github.com/CrystalEye42/eval-safety)

[2024-01-12] â„ï¸ Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender [[Paper]](https://arxiv.org/pdf/2401.06561.pdf) [[Code]](https://github.com/alphadl/SafeLLM_with_IntentionAnalysis)

[2023-12-30] ğŸ„The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness [[Paper]](https://arxiv.org/pdf/2401.00287.pdf)

[2023-12-29] ğŸ„ Jatmo: Prompt Injection Defense by Task-Specific Finetuning [[Paper]](https://arxiv.org/pdf/2312.17673.pdf)

[2023-12-12] ğŸ„Defending Chatgpt against jailbreak attack via self-reminders [[Paper]](https://www.nature.com/articles/s42256-023-00765-8)

[2023-12-12] ğŸ„ Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack [[Paper]](https://arxiv.org/pdf/2312.06924.pdf) [[Code]](https://github.com/FYYFU/SafetyAlignNLP)

[2023-11-15] ğŸ¦ƒDefending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization[[Paper]](https://arxiv.org/pdf/2311.09096.pdf)

[2023-11-13] ğŸ¦ƒ MART: Improving LLM Safety with Multi-round Automatic Red-Teaming [[Paper](https://arxiv.org/pdf/2311.07689.pdf)]

[2023-10-10] ğŸƒ Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations [[Paper]](https://arxiv.org/pdf/2310.06387.pdf)

[2023-10-09] ğŸƒ SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese [[Paper]](https://arxiv.org/pdf/2310.05818.pdf) [[Code]](https://www.cluebenchmarks.com/)

[2023-10-05] ğŸƒ SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks [[Paper]](https://arxiv.org/pdf/2310.03684.pdf) [[Code]](https://github.com/arobey1/smooth-llm)

[2023-09-18] ğŸ‚ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM [[Paper]](https://arxiv.org/pdf/2309.14348.pdf) [[Code]](https://github.com/AAAAAAsuka/llm_defends)

[2023-09-13] ğŸ‚ RAIN: Your Language Models Can Align Themselves without Finetuning [[Paper]](https://arxiv.org/pdf/2309.07124.pdf) [[Code]](https://github.com/SafeAILab/RAIN)

[2023-09-06] ğŸ‚ Certifying LLM Safety against Adversarial Prompting [[Paper]](https://arxiv.org/pdf/2309.02705.pdf) [[Code]](https://github.com/aounon/certified-llm-safety)

[2023-09-01] ğŸ‚ Baseline Defenses for Adversarial Attacks Against Aligned Language Models [[Paper]](https://arxiv.org/pdf/2309.00614.pdf) [[Code]](https://github.com/neelsjain/baseline-defenses)

[2023-08-27] ğŸŒ´ Detecting Language Model Attacks with Perplexity [[Paper](https://arxiv.org/pdf/2308.14132.pdf)]

[2023-08-18] ğŸŒ´ Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment [[Paper]](https://arxiv.org/pdf/2308.09662.pdf) [[Code]](https://github.com/declare-lab/red-instruct)

[2023-08-14] ğŸŒ´ LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked [[Paper]](https://arxiv.org/pdf/2308.07308.pdf)

[2023-08-02] ğŸŒ´ XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models [[Paper]](https://arxiv.org/pdf/2308.01263.pdf) [[Code]](https://github.com/paul-rottger/exaggerated-safety)

[2023-07-17] ğŸ¦ Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models [[Paper]](https://arxiv.org/pdf/2307.08487.pdf) [[Code]](https://github.com/qiuhuachuan/latent-jailbreak)
