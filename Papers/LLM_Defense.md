# Timeline ğŸš€ 
(Jan:â„ï¸, Feb:ğŸ’•, Mar:ğŸŒ±, Apr:ğŸŒ¸, May:ğŸŒº, Jun:â˜€ï¸, Jul:ğŸ¦, Aug:ğŸŒ´, Sep:ğŸ‚, Oct:ğŸƒ, Nov:ğŸ¦ƒ, Dec:ğŸ„)

[2024-03-02] ğŸŒ± AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks [[Paper](https://arxiv.org/pdf/2403.04783)][[Code](https://github.com/XHMY/AutoDefense)]

[2024-03-01] ğŸŒ± Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes [[Paper](https://arxiv.org/pdf/2403.00867.pdf)][[Code](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense)]

[2024-02-25] ğŸ’• Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing [[Paper](https://arxiv.org/pdf/2402.16192.pdf)][[Code](https://github.com/UCSB-NLP-Chang/SemanticSmooth)]

[2024-02-24] ğŸ’• LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper [[Paper](https://arxiv.org/pdf/2402.15727.pdf)]

[2024-02-23] ğŸ’• Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement [[Paper](https://arxiv.org/pdf/2402.15180.pdf)][[Code](https://anonymous.4open.science/r/refine-a-broken-4E03/)]

[2024-02-22] ğŸ’• Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment [[Paper](https://arxiv.org/pdf/2402.14968.pdf)]

[2024-02-21] ğŸ’• LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study [[Paper](https://arxiv.org/pdf/2402.13457.pdf)][[Code](https://sites.google.com/view/llmcomprehensive/home)]

[2024-02-21] ğŸ’• GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis [[Paper](https://arxiv.org/pdf/2402.13494.pdf)][[Code](https://github.com/xyq7/GradSafe)]

[2024-02-14] ğŸ’• SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding [[Paper](https://arxiv.org/pdf/2402.08983.pdf)][[Code](https://github.com/uw-nsl/SafeDecoding)]

[2024-01-31] â„ï¸ Prompt-Driven LLM Safeguarding via Directed Representation Optimization [[Paper](https://arxiv.org/pdf/2401.18018.pdf)][[Code](https://github.com/chujiezheng/LLM-Safeguard)]

[2024-01-30] â„ï¸ Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks [[Paper](https://arxiv.org/pdf/2401.17263.pdf)][[Code](https://github.com/andyz245/rpo)]

[2024-01-19] â„ï¸ Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning [[Paper](https://arxiv.org/pdf/2401.10862.pdf)][[Code](https://github.com/CrystalEye42/eval-safety)]

[2024-01-12] â„ï¸ Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender [[Paper](https://arxiv.org/pdf/2401.06561.pdf)][[Code](https://github.com/alphadl/SafeLLM_with_IntentionAnalysis)]

[2023-12-12] ğŸ„ Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack [[Paper](https://arxiv.org/pdf/2312.06924.pdf)][[Code](https://github.com/FYYFU/SafetyAlignNLP)]

[2023-11-20] ğŸ¦ƒ Evil Geniuses: Delving into the Safety of LLM-based Agents [[Paper](https://arxiv.org/pdf/2311.11855.pdf)][[Code](https://github.com/evil-genius-project/evil-genius)]

[2023-11-13] ğŸ¦ƒ MART: Improving LLM Safety with Multi-round Automatic Red-Teaming [[Paper](https://arxiv.org/pdf/2311.07689.pdf)]

